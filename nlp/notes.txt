1. TOWARDS UNIVERSAL PARAPHRASTIC SENTENCE EMBEDDINGS - https://arxiv.org/pdf/1511.08198.pdf
Task: Learn embeddings representations optimized for Textual similarity Tasks
Approach: Apart from modeling on diverse textual similarity datasets, they use publicly available dataset for synonyms such as "PPDB" etc. to tune word representations. The paper release novel word-embeddings publicly which can be used for getting representative "sentence vector" in unsupervised manner. Their simpler model works better than LSTM based representations for some of the textual similarity datasets.
Author page: https://www.cs.cmu.edu/~jwieting/


2. Skip-Thought Vectors - https://arxiv.org/abs/1506.06726
Tasks: unsupervised learning of a generic, distributed sentence encoder
Approach: Similar analogy to skip-gram but rather than doing it on word-level, they work on sentence level i.e. Given a sentence, encode it and generate surrounding sentences during decoding phrase. Paper also describes way for getting representation for unknown token not seen during training time.
Evaluation:  semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets
Doubts:
Pending: Read Paper
Addtional Notes: https://gist.github.com/shagunsodhani/4a4eb32de8cabf21bda9a4ada15c46e8


3. Loss in Translation:Learning Bilingual Word Mapping with a Retrieval Criterion - https://arxiv.org/pdf/1804.07745.pdf
Idea: Mapping two different language embeddings to same space. generally mapping src embeddings (ex. fr) to target space (eg. en)
Approach works on language pairs only. Different mapping (W) is learnt for each language pair. having W as orthogonal matrix improves translation accuracy significantly.
Previous approaches: use of l2_loss(W*x - y) is used during training. x and y are known monolingual embeddings of src_word and corresponding tgt_word (en). W (d*d matrix) is learnt during training with MSE as loss function. This approach suffers from hubness problem during inference time (getting nearest target word for a given source word) since there are words (hubs) which are close of lot of words. Smith (2017) proposed a solution of using Inverted softmax where rather than taking softmax over all target word in denominotor, do it over source words. Basically hubs are the target_words which appear as nearest neighbours to many src_words. so rather than considering whether source word traslates to target word, we consider whether a target word translated back to source word thus taking normalization over source words makes denominator bigger for hubs thus they are not selected as target_word during inference time (find correponding translated word for a src_word).
Problem with above approach is that we don't follow the same mechanism during training and testing. this is not right. so use CSCL. Learning and inference are consistent.
RCSCL: W as orthogonal matrix and word vectors are l2-normalized. RCSCL is better than MUSE embeddings.
Extended normalization: instead of computing k-nearest neighbour over annotated words, do it over all word vocabulary. Random negative sampling. it improves word translation accuracy ~3-4%


4. UNSUPERVISED HYPERALIGNMENT FOR MULTILINGUAL WORD EMBEDDINGS - https://arxiv.org/pdf/1811.01124.pdf
Idea: Jointly align N set of word vectors to a common target space (ex. en) i.e. Learning mapping jointly from more than one language embedding space to a common (en, referred as pivot) space. Learning jointly improve indirect word translation greatly.
-> Assignment matrix P (n*n matrix), and mapping Q (d*d matrix), X&Y (n*d matrices). P(i) and Q(i) is language specific. Q(i) is mapping from ith language to pivot
-> solving Q while P is fixed: Orthogonal procrustes problem: O(d^3), solving P (assignment matrix) while Q is fixed: hungarian algorithm: O(n^3) : very expensive
Regularizing with -ve entropy leads to Sinkhorn algorithm: O(n^2).
-> A good initialization of P helps in finding local minima. To find good initialization for P, they use entropic regularization of GW (Gromov-Wassertein) problem. This is used in recent papers too.
-> Novelity: consider all pairs of languages (i,j) rather than only (i, pivot). This notion improves indirect word translation while maintaining similar peroformance on direct translation to pivot. Indirect word translation: src -> en -> tgt.
-> In loss function: use of alpha(i,j) reflects (weight) improtance of alignment quality between language i and j; alpha(i,j) = N if either of i or j is pivot else 1
-> model loss function: wieghted (alpha) RCSCL loss with multialignment objective (i,j); training: for first few epochs, l2 loss is optimized and then switch to RCSCL loss. for l2 loss use Sinkhorn algorithm. Sinkhorn is an OT algorithm.
-> diff batch_size and lr for initial and later epochs. Initially use Sinkhorn algorithm for assignment matrix and then for efficiency, take max over each row.
dataste: MUSE test dataset, learning joint alignment over 11 languages,
-> Results:
1. Triplet alignment: presence of another language doesn't help in case of direct translation. Interestingly, presence of another language improves performancefor indirect translation compared to without another language presence for direct translation.
2. Multilingual Alignment: For direct translation, it preserves the performance (for unsupervised) and improve performance by ~4% for indirect translation. Total 11 languges are considered for this task which are from same as well as different family. Some languages are distant from English.

Observation:
-> uniform weighting (alpha) improves indirect translation at cost of reduced performance on direct translation. Paper doesn't use uniform weighting.
-> as number of languages increases (6 to 11), performance only reduces by 1%, so this can be scaled to multple languages. Not sure for hundreds on languages though.
-> Model training is quite fast and can be done on CPU.

Doubts: what is the training data? Are any bilingual dictionaries being used during training? If yes, what is the use of P (assignment matrix)?








